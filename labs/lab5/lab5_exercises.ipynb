{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2023-2024 - UMONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-variance tradeoff."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, we will experiment with the notorious 'Bias-Variance trade-off', a highly important concept in machine learning. In the context of a regression task, you've seen in the course that there exists an elegant decomposition of the MSE, which allows to illustrate this concept nicely. Contrary to previous labs in which we worked with real datasets, we will place ourself in a context where we know the true data generating process. This will enable us to estimate the bias and the variance of a given model.** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import the necessary libraries.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generating process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Create the functions `fx` and `fy` that define the following data generating process:**\n",
    "\n",
    "$$X \\sim \\mathcal{U}(-4, 4)$$\n",
    "\n",
    "$$Y \\sim f(X) + \\epsilon$$\n",
    "\n",
    "**where $\\mathcal{U}(a, b)$ denotes a uniform distribution between $a$ and $b$, $f(X) = \\text{sin}(X) + X / 3$, and $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$ is a Gaussian noise with a normal distribution of mean 0 and standard deviation 0.5.**\n",
    "\n",
    "**The parameter of `fx` should be:**\n",
    "- **`size`, the size of the dataset which is sampled**\n",
    "\n",
    "**The parameters of `fy` should be:**\n",
    "- **`x`: an array of conditioning values corresponding to the random variable $X$**\n",
    "- **`add_noise=True`: whether the noise $\\epsilon$ should be added to $\\text{sin}(X)$ (true by default).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define the size of the training dataset to `n_train = 30`.**\n",
    "\n",
    "**We also define a set of values `x_test` on which we will evaluate the bias-variance tradeoff.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 30\n",
    "x_test = np.linspace(-4, 4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We plot a training dataset sampled using the data generating process. We also plot the function $f(x)$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fx(n_train)\n",
    "y_train = fy(x_train)\n",
    "y_test = fy(x_test)\n",
    "y_test_mean = fy(x_test, add_noise=False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test_mean, label='f(x)', color='green', ls='--')\n",
    "ax.scatter(x_train, y_train, label='Train points', color='blue')\n",
    "ax.set(xlabel='x', ylabel='y')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Our goal is to predict the values of $Y$ given the conditioning values $x_\\text{test}$.**\n",
    "- **Fit a linear regression model on the training dataset.**\n",
    "- **Predict `y_pred` on the test dataset `(x_test, y_test)` and report the mean squared error (MSE).**\n",
    "**Note that scikit-learn expects inputs of shape `(n_train, n_features)`, where the second dimension is 1 in our case since $X$ is unidimensional.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We draw the previous figure, with an added regression line corresponding to the predictions of the linear regression model on `x_test`. Does the model appear to be a good fit to the data? How do you link this observation to the bias-variance tradeoff?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test_mean, label='f(x)', color='green', ls='--')\n",
    "ax.scatter(x_train, y_train, label='Train points', color='blue')\n",
    "ax.plot(x_test, y_pred, label='Model', color='red')\n",
    "ax.set(xlabel='x', ylabel='y')\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) We will now decompose our models' test MSE into a bias, variance and Bayes error term. Recall from the course that the MSE decomposition is given by:**\n",
    "\n",
    "$$\\mathbb{E}_{\\mathcal{D},y,x}[(y-g(x))^2)] = \\underbrace{\\mathbb{E}_x[\\text{Var}(y|x)]}_{\\text{Bayes Error}} + \\underbrace{\\mathbb{E}_x[(f(x)-\\mathbb{E}_\\mathcal{D}[g(x)])^2]}_{\\text{Bias}} + \\underbrace{\\mathbb{E}_x[\\text{Var}(g(x))]}_{\\text{Variance}}$$\n",
    "\n",
    "**where $\\mathcal{D}$ is a training set from the data generating process.**\n",
    "\n",
    "**To estimate each term of the decomposition, we will:**\n",
    "- **Fix a set of features $\\{x_i^\\text{test}\\}_{i=1}^{n_\\text{test}}$ on which we will evaluate the bias-variance tradeoff.**\n",
    "- **For $j=1,\\dots,J$, we will simulate the modeling pipeline:**\n",
    "    - **Sample a dataset $\\mathcal{D}_{j,\\text{train}}$ containing $n_\\text{train}$ observations $\\{(x_{i,j}^\\text{train}, y_{i,j}^\\text{train})\\}_{i=1}^{n_\\text{train}}$.**\n",
    "    - **Fit the model to $\\mathcal{D}_{j,\\text{train}}$, yielding $g_j$.**\n",
    "    - **Sample a dataset $\\mathcal{D}_{j,\\text{test}}$ containing $n_\\text{test}$ observations $\\{(x_{i}^\\text{test}, y_{i,j}^\\text{test})\\}_{i=1}^{n_\\text{test}}$.**\n",
    "    - **Make predictions on $\\mathcal{D}_{j,\\text{test}}$ using $g_j$.**\n",
    "\n",
    "**The above steps will allow us to obtain an estimate of the Bias Variance of the fitted models:**\n",
    "$$\\mathbb{E}_{\\mathcal{D},y,x}[(y-g(x))^2)] \\simeq \\underbrace{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_\\text{test}} \\Big[\\frac{1}{J} \\sum_{j=1}^J (y^\\text{test}_{i,j} - \\bar{y_i})^2\\Big]}_{\\text{Bayes Error}} + \\underbrace{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_\\text{test}} \\Big[(f(x_i^\\text{test}) - \\bar{g}(x_i^\\text{test}))^2\\Big] }_{\\text{Bias}} + \\underbrace{\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\Big[\\frac{1}{J}\\sum_{j=1}^J(g_j(x_i^\\text{test}) - \\bar{g}(x_i^\\text{test}))^2 \\Big]}_{\\text{Variance}},$$\n",
    "\n",
    "**where $\\bar{y}_i = \\frac{1}{J} \\sum_{j=1}^J y_{i,j}^\\text{test}$ and $\\bar{g}(x_i^\\text{test}) = \\frac{1}{J}\\sum_{j=1}^J g_j(x_i^\\text{test})$.**\n",
    "\n",
    "**To this end, complete the functions `simulate_modeling_pipeline` and `bias_variance_estimator` below. The parameters are:**\n",
    "- `model`: the scikit-learn model.\n",
    "- `x_test`: the array of features $\\{x_i^\\text{test}\\}_{i=1}^{n_\\text{test}}$.\n",
    "- `n_train`, `fx` and `fy`: they define the data generating process of $\\mathcal{D}_{j,\\text{test}} = \\{(x_{i,j}^\\text{train}, y_{i,j}^\\text{train})\\}_{i=1}^{n_\\text{train}}$.\n",
    "- `n_datasets`: the number of datasets $J$ to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_modeling_pipeline(model, x_test, n_train, fx, fy):\n",
    "    # The training dataset is generated using `fx` and `fy`\n",
    "    # TODO: x_train = ...\n",
    "    # TODO: y_train = ...\n",
    "    # The test target is generated using `fy` and the provided `x_test`\n",
    "    # TODO: y_test = ...\n",
    "    # The model is trained on the training dataset and used to predict the test target `y_pred`\n",
    "    # TODO: fit the model\n",
    "    # TODO: predict the test target\n",
    "    return y_test, y_pred\n",
    "\n",
    "\n",
    "def bias_variance_estimator(model, x_test, n_train, fx, fy, n_datasets):\n",
    "    y_test, y_pred = [], []\n",
    "    # We simulate the modeling pipeline on `n_datasets` datasets\n",
    "    for _ in range(n_datasets):\n",
    "        y_test_i, y_pred_i = simulate_modeling_pipeline(model, x_test, n_train, fx, fy)\n",
    "        y_test.append(y_test_i)\n",
    "        y_pred.append(y_pred_i)\n",
    "    \n",
    "    # We get targets and predictions as numpy arrays of shape (n_datasets, n_test)\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # We compute the MSE, bias, variance and Bayes error of each test point\n",
    "    # Each of these quantities must be an array of shape (n_test,)\n",
    "    # The suffix _x indicates that the estimation is conditional on x\n",
    "    # TODO: mse_x = ...\n",
    "    # TODO: bayes_error_x = ...\n",
    "    # TODO: bias_x = ...\n",
    "    # TODO: variance_x = ...\n",
    "    return mse_x, bayes_error_x, bias_x, variance_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Call `bias_variance_estimator` and verify that the decomposition $\\text{MSE} = \\text{Bayes Error} + \\text{Bias} + \\text{Variance}$ is approximately correct when $J$ is sufficiently large. Does the value obtained for the Bayes Error make sense?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_x, bayes_error_x, bias_x, variance_x = bias_variance_estimator(model, x_test, n_train, fx, fy, n_datasets=1000)\n",
    "mse, bayes_error, bias, variance = mse_x.mean(), bayes_error_x.mean(), bias_x.mean(), variance_x.mean()\n",
    "print(f'MSE = {mse:.5f} ~= {bayes_error + bias + variance:.5f} = {bayes_error:.5f} + {bias:.5f} + {variance:.5f} = Bayes Error + Bias + Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Based on the results of `bias_variance_estimator`, plot the MSE, Bayes error, bias and variance conditionally to $X$, for each $x$ in `x_test`. How do you interpret the plot?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will now study how the bias and variance of a model evolve when varying its complexity. To this end, we will use polynomial regression models with varying degrees. Examples of predictions are given below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit models of varying degrees on the training dataset\n",
    "models = []\n",
    "degrees = [1, 2, 3, 5, 7]\n",
    "for degree in degrees:\n",
    "    model = PolynomialRegression(degree, fit_intercept=True)\n",
    "    model.fit(x_train[:, np.newaxis], y_train)\n",
    "    models.append(model)\n",
    "# We plot the predictions of the models\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test_mean, label='f(x)', color='green', ls='--')\n",
    "ax.scatter(x_train, y_train, label='Train points', color='blue')\n",
    "for model in models:\n",
    "    degree = model.get_params()['polynomialfeatures__degree']\n",
    "    y_pred = model.predict(x_test[:, np.newaxis])\n",
    "    ax.plot(x_test, y_pred, label=f'Degree {degree}')\n",
    "ax.set(xlabel='x', ylabel='y')\n",
    "fig.legend(loc='lower center', bbox_to_anchor=(0.5, 1), frameon=True, ncol=4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) For polynomial regression models with degrees ranging from 1 to 6, can you collect the MSE, Bayes error, bias, and variance, and plot them as function of the degree? What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $K$-nearest neighbors regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we investigate another class of models, $K$-nearest neighbors regression models.**\n",
    "\n",
    "**Given a feature $x$, these models select the $K$ points $\\{(x_k, y_k)\\}_{k=1}^K$ whose feature are closest to $x$ and predict the average target among these $K$ points. Examples of predictions are given below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "n_neighbors_list = [1, 3, 10]\n",
    "for n_neighbors_list in n_neighbors_list:\n",
    "    model = KNeighborsRegressor(n_neighbors=n_neighbors_list)\n",
    "    model.fit(x_train[:, np.newaxis], y_train)\n",
    "    models.append(model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, y_test_mean, label='f(x)', color='green', ls='--')\n",
    "ax.scatter(x_train, y_train, label='Train points', color='blue')\n",
    "\n",
    "for model in models:\n",
    "    K = model.get_params()['n_neighbors']\n",
    "    y_pred = model.predict(x_test[:, np.newaxis])\n",
    "    ax.plot(x_test, y_pred, label=f'{K}-neighbors')\n",
    "\n",
    "ax.set(xlabel='x', ylabel='y')\n",
    "fig.legend(loc='lower center', bbox_to_anchor=(0.5, 1), frameon=True, ncol=4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Repeat the same exercise as in 6), but for the number of neighbors in a `KNeighborsRegressor`.**\n",
    "\n",
    "**Are the evolution of the bias, variance and Bayes error term expected ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional experiments\n",
    "\n",
    "**Based on the code above, can you answer the following questions?**\n",
    "- **How does the bias-variance tradeoff change when varying the number of training samples?**\n",
    "- **How does the bias-variance tradeoff change with a more complex data generating process?**\n",
    "- **Can you plot the MSE, Bayes error, bias and variance in function of $x$ for different polynomials and $K$-nearest neighbors regression models? How do you interpret them?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfe0e2b9b67a2fcef00021a1d2a516837bff30cf713e434b27f739f4afd30381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
