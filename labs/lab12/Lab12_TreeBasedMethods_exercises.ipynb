{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning 2023-2024 - UMONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP2BHEtuQ26i"
      },
      "source": [
        "# Tree-based methods \n",
        "\n",
        "In this lab, our objective is to predict whether female patients suffer from diabetes based on a series of medical attributes. To do so, we will apply several tree-based methods to the Pima Indians dataset.\n",
        "\n",
        "Here is a description of the dataset's attributes:\n",
        "  - **Num_pregnant** : The number of pregnancies the patient had. \n",
        "  - **glucose_con** : Patient's plasma glucose concentration.\n",
        "  - **blood_pressure** : Patient's dialostic blood pressure (mmHg).\n",
        "  - **triceps_thickness** : Patient's triceps skin-fold thickness (mm).\n",
        "  - **insulin** : Patient's 2-h serum insulin (mu U/mL).\n",
        "  - **bmi** : Patient's body mass index (kg/m^2).\n",
        "  - **dpf** : Patient's diabetes pedigree function.\n",
        "  - **age** : Patient's age. \n",
        "  - **diabetes** : Whether the patient has diabetes (1) or not (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWWFCoLHEeA4"
      },
      "source": [
        "**Load the necessary libraries** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pa-8tr1GBLlE"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, BaggingClassifier, BaggingRegressor\n",
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHo7LMdIEhmQ"
      },
      "source": [
        "**1) Load the dataset, get its general information, and check for missing values.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mje6W8ucZYvi"
      },
      "source": [
        "# Decision Trees"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qKdTxEtuE47K"
      },
      "source": [
        "**2) Select 'diabetes' as the target variable, and all the remaining columns as predictors.**\n",
        "\n",
        "**Create a pipeline containing the preprocessing steps (missing values imputer, scaler, ...) and a `DecisionTreeClassifier` with a maximum depth set to 3 (through the `max_depth` argument). Use the entropy as split criterion.** \n",
        "- **Do you think scaling the variables is necessary?**\n",
        "\n",
        "\n",
        "**Fit this pipeline to the data (do not split the dataset for the time being), and plot the decision tree. How do you interpret it?** \n",
        "\n",
        "**You'll need the `plot_tree` method from the sklearn library. You can access the pipeline's classifier using the `named_steps['classifier']` attributes. You'll also need to pass the features' (predictors) names to the function using the `features_names` argument.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hHndFhA_J4QK"
      },
      "source": [
        "**3) Let's see how the model's performance evolve as a function of the tree's maximum depth.**\n",
        "\n",
        "**To this end, apply the following steps:**\n",
        "- **Split the dataset into a training and a test set following a 0.8/0.2 partition**\n",
        "- **For maximum depths varying from 1 to 20, fit a `DecisionTreeClassifier` to the *training* data using a 10 folds cross-validation with the AUROC as metric.**\n",
        "- **Plot the means of the training and validation AUROCS across each folds as a function of the maximum depth.**\n",
        "- **Compute the one standard error of the means at each depth, and add it to the plot as a shaded grey area around the means.**\n",
        "    - **What can you conclude regarding the model's performance, as well as the uncertainty for the in-sample and out-of-sample AUROC estimates ?**\n",
        "- **Identify which depth would lead a priori to the best model's out-of-sample performance. Using this depth, fit a decision tree to the training split and report the training AUROC and the test AUROC.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdvKtPEa7KRe"
      },
      "source": [
        "# Bagging "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EwkZbxViZ_Y0"
      },
      "source": [
        "**4) Implement your own bagging algorithm by fitting a decision tree to each bootstrap sample. To this end, perform the following steps:**\n",
        "- **Draw 30 bootstrap samples from the training set with replacement. Each bootstrap sample should contain the same number of observations as in the training set. Use the `resample()` method of scikit-learn.** \n",
        "- **For each bootstrap sample, do:**\n",
        "  - **Fit a `DecisionTreeClassifier` to the bootstrap sample. Reuse the pipeline defined previously. The maximum depth of each tree should be fixed to 5.**\n",
        "  - **Using the fitted decision tree, predict the class and the probabilities on the test set, and save them in a list.**\n",
        "- **You will now use two different aggregation strategies to get a single prediction from the ensemble:** \n",
        "  - **Majority vote strategy: predict the class that was predicted the most by each tree separately.**\n",
        "  - **Average probability strategy: predict the class whose average probability across each tree is the highest.** \n",
        "- **Plot the confusion matrix of the predictions for both aggregation strategies separately.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlMQavsWhGO6"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1_648UmlJrd"
      },
      "source": [
        "**5) Perform a `RandomizedSearchCV` on a specified grid of hyper-parameters to find the best configuration for a `RandomForestClassifier`. Set the scoring function as the AUROC and limit the number of combination to try to 10.**\n",
        "\n",
        "**Fit the best model found in the previous procedure to the training data, and predict on the test set. Report the test AUROC and display the ROC curve.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpJYcRiqfa0R"
      },
      "source": [
        "# Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbLLa_mDmpV2"
      },
      "source": [
        "**7) Fit a `GradientBoostingClassifier` to the training data, and report the training and test AUROCs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iMGufJ8XlEV"
      },
      "source": [
        "**8) For a `DecisionTreeClassifier`, a `BaggingClassifier`, a `RandomForestClassifier` and a `GradientBoostingClassifier`, perform a `RandomizedSearchCV` on a predefined grid of hyper-parameters. Amongst all models and hyper-parameters combinations, select the best model and report the best *validation* AUROC. The random search should be performed on the *training* data, and you can set the number of combinations to try per model to 5.**\n",
        "\n",
        "**For the best model found, report the training and test AUROC's, and display the training and test ROC curves**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEvdTtjsrCEf"
      },
      "source": [
        "# Regression "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "--wfsvsUYo3C"
      },
      "source": [
        "**9) Select *bmi* as the target variable and all the remaining columns in the dataframe as the predictors, at the exception to *diabetes*. Split your dataset into a training and test set, fit a `DecisionTreeRegressor` to the training data, and report the MSE on the training and test sets. What do you observe ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMUd7Kyqa6S4"
      },
      "source": [
        "**10) For a `DecisionTreeRegressor`, a `BaggingRegressor`, a `RandomForestRegressor` and a `GradientBoostingRegressor`, perform a `RandomizedSearchCV` on a predefined grid of hyper-parameters. Amongst all models and hyper-parameters combinations, select the best model and report the best *validation* MSE. The random search should be performed on the *training* data, and you can set the number of combinations to try per model to 5.**\n",
        "\n",
        "**For the best model found, report the training and test MSE.**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab11_solutions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
