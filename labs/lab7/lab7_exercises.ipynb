{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2023-2024 - UMONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab about linear regression, we'll be working with the library [statsmodels](https://www.statsmodels.org/stable/index.html), which provides numerous classes and functions for the estimation of statistical models.**\n",
    "\n",
    "**The dataset that we'll be considering is [Diamonds](https://www.kaggle.com/datasets/shivam2503/diamonds), which contains several characteristics of a series of diamonds, such as their dimensions, the quality of their cuts, their prices, etc... The goal of the lab will be to define linear regression models to best estimate diamonds prices using a bunch of predictor variables, and to understand the meaning of the obtained coefficients.**\n",
    "\n",
    "**Dataset's column information:**\n",
    "\n",
    "- 'price': price in US dollars.\n",
    "- 'carat': weight of the diamond. \n",
    "- 'cut': quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "- 'color': diamond's color's, from J (worst) to D (best).\n",
    "- 'clarity': how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "- 'x': length in mm.\n",
    "- 'y': width in mm. \n",
    "- 'z': height in mm.\n",
    "- 'table': width of top of the diamond relative to its widest point. \n",
    "- 'depth' = 2z/(x+y) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We load the 'Diamond' dataset as a Dataframe. We also add two columns with noise in order to experiment with variable selection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/diamonds.csv', index_col=0)\n",
    "# Add a column with noise\n",
    "df['noise'] = np.random.normal(0, 1, len(df))\n",
    "df['x_noise'] = df['x'] + np.random.normal(0, 1, len(df))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Check the number of observations, the number of columns, the column types, and the number of missing values per columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Set the data type of the categorical variables to 'category'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Make a scatter plot of the variable 'price' along the y-axis and the variable 'x' along the x-axis. Do the same for the variables 'y' and 'z'.**\n",
    "\n",
    "**Do you notice anything special?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Following your observations from the previous question, what would you do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using statsmodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Using the [`dmatrices`](https://patsy.readthedocs.io/en/latest/API-reference.html#patsy.dmatrices) function of the patsy library and the [`OLS`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html) class of the statsmodels library, fit a linear regression model to the diamonds' price using the variable 'x' as your predictor. Then, based on the results given by the [`summary`](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.summary.html) method:** \n",
    "\n",
    "- **5.1) Report the coefficient of determination and the adjusted coefficient of determination of the model.**\n",
    "- **5.2) How would you write the linear regression model?**\n",
    "- **5.3) Are the fitted coefficients statistically significant at the 5% significance level?**\n",
    "- **5.4) How would you obtain the confidence intervals?**\n",
    "- **5.5) How do you interpret the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Compute the linear regression coefficients and intercept using the formula $\\hat{\\beta} = (X^T X)^{-1} X^T y$. Check that the obtained coeffients are the same than in the previous exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Fit the diamonds' price to the variable 'cut'. Note that 'cut' is a categorical variable, which we learned to preprocess in previous labs. When calling `dmatrices`, categorical variables will automatically be converted into dummy variables. You can also use the syntax [`C(variable)`](https://patsy.readthedocs.io/en/latest/categorical-coding.html) for more flexibility.**\n",
    "- **7.1) Does the model appear to be a better fit than with the variable 'x'?**\n",
    "- **7.2) How would you write the linear regression model now?**\n",
    "- **7.3) Are all coefficients statistically significant at the 5% significance level?**\n",
    "- **7.4) How do you interpret the model ? Does the intercept have a meaning?**\n",
    "- **7.5) Using `sns.boxenplot`, compare the distributions of the variable 'price' for the different values of 'cut'. Is it coherent to the model results?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) Fit a linear regression model to the variable 'price' using 'x' and 'cut' as predictors. Based on [patsy's formulas](https://patsy.readthedocs.io/en/latest/formulas.html), add an interaction term between 'x' and 'cut'.**\n",
    "- **8.1) How does the model write now?**\n",
    "- **8.2) How do you interpret the model for a 'Good' cut diamond?**\n",
    "- **8.3) Are all coefficients statistically significant ? If no, what does it mean?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Fit a linear regression model to the variable 'price' using all predictors.**\n",
    "- **9.1) Derive the value of the Bayesian Information Criterion (BIC) and check that it corresponds to the value reported by the model's summary**  \n",
    "- **9.2) Does the model appear to be a better fit than the ones using only 'x' and 'cut' as predictors? Which criterion would you look at?**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10) The function `forward_selection` below implements a forward stepwise selection of predictors for linear regression. Make sure to understand each of its steps, and how they relate to the algorithm seen in class.**\n",
    "\n",
    "**Using this function, find the best subset of predictors using the BIC as selection critierion. Redo the same by using the AIC and $R^2$ as selection criterion. Do you notice any differences ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(df, target, criterion='BIC'):\n",
    "    assert criterion in ['BIC', 'AIC', 'R2'], 'Unknown criterion !'\n",
    "    # Iteratively, select the best feature to add to the model\n",
    "    candidates = set(df.columns) - {target}\n",
    "    ordered_predictors = []\n",
    "    while len(candidates) > 0:\n",
    "        rsquared_dict = {}\n",
    "        for predictor in candidates:\n",
    "            res = fit(df, ordered_predictors + [predictor], target)\n",
    "            rsquared_dict[predictor] = res.rsquared\n",
    "        best_predictor = max(rsquared_dict, key=rsquared_dict.get)\n",
    "        ordered_predictors = ordered_predictors + [best_predictor]\n",
    "        candidates.remove(best_predictor)\n",
    "    print(f'Best to worst predictors: {ordered_predictors}')\n",
    "    # Compute the scores obtained by adding each feature\n",
    "    current_features = []\n",
    "    scores = []\n",
    "    for feature in ordered_predictors:\n",
    "        current_features += [feature]\n",
    "        res = fit(df, current_features, target)\n",
    "        if criterion == 'BIC':\n",
    "            scores.append(res.bic)\n",
    "        elif criterion == 'AIC':\n",
    "            scores.append(res.aic)\n",
    "        elif criterion == 'R2':\n",
    "            scores.append(res.rsquared)\n",
    "    # Select the features that optimize the criterion\n",
    "    if criterion in ['BIC', 'AIC']:\n",
    "        num_final_features_to_keep = np.argmin(scores)\n",
    "    else:\n",
    "        num_final_features_to_keep = np.argmax(scores)\n",
    "    final_features_to_keep = ordered_predictors[:num_final_features_to_keep + 1]\n",
    "    best_final_score = scores[num_final_features_to_keep]\n",
    "    return final_features_to_keep, best_final_score\n",
    "\n",
    "def fit(df, features_to_try, target):\n",
    "    formula = get_formula(features_to_try, target)\n",
    "    y, X = dmatrices(formula, data=df, return_type='dataframe')\n",
    "    mod = sm.OLS(y, X)\n",
    "    res = mod.fit()\n",
    "    return res\n",
    "\n",
    "def get_formula(features_to_try, target):\n",
    "    return target + ' ~ ' + ' + '.join(features_to_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfe0e2b9b67a2fcef00021a1d2a516837bff30cf713e434b27f739f4afd30381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
