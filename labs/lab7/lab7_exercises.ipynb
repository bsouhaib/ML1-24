{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2023-2024 - UMONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab about linear regression, we'll be working with the library [statsmodels](https://www.statsmodels.org/stable/index.html), which provides numerous classes and functions for the estimation of statistical models.**\n",
    "\n",
    "**The dataset that we'll be considering is [Diamonds](https://www.kaggle.com/datasets/shivam2503/diamonds), which contains several characteristics of a series of diamonds, such as their dimensions, the quality of their cuts, their prices, etc... The goal of the lab will be to define linear regression models to best estimate diamonds prices using a bunch of predictor variables, and to understand the meaning of the obtained coefficients.**\n",
    "\n",
    "**Dataset's column information:**\n",
    "\n",
    "- 'price': price in US dollars.\n",
    "- 'carat': weight of the diamond. \n",
    "- 'cut': quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
    "- 'color': diamond's color's, from J (worst) to D (best).\n",
    "- 'clarity': how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
    "- 'x': length in mm.\n",
    "- 'y': width in mm. \n",
    "- 'z': height in mm.\n",
    "- 'table': width of top of the diamond relative to its widest point. \n",
    "- 'depth' = 2z/(x+y) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We load the 'Diamond' dataset as a Dataframe. We also add two columns with noise in order to experiment with variable selection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/diamonds.csv', index_col=0)\n",
    "# Add a column with noise\n",
    "df['noise'] = np.random.normal(0, 1, len(df))\n",
    "df['x_noise'] = df['x'] + np.random.normal(0, 1, len(df))\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Check the number of observations, the number of columns, the column types, and the number of missing values per columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Set the data type of the categorical variables to 'category'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Make a scatter plot of the variable 'price' along the y-axis and the variable 'x' along the x-axis. Do the same for the variables 'y' and 'z'.**\n",
    "\n",
    "**Do you notice anything special?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations of 'x', 'y' and 'z' take on the values 0. Considering that these variables represent dimensions, such values are impossible and likely result from an encoding mistake. Moreover, for 'y' and 'z', we observe some values greater than 30mm, which are far off from the rest of the distribution. These observations are called outliers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Following your observations from the previous question, what would you do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression using statsmodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Using the [`dmatrices`](https://patsy.readthedocs.io/en/latest/API-reference.html#patsy.dmatrices) function of the patsy library and the [`OLS`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html) class of the statsmodels library, fit a linear regression model to the diamonds' price using the variable 'x' as your predictor. Then, based on the results given by the [`summary`](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.summary.html) method:** \n",
    "\n",
    "- **5.1) Report the coefficient of determination and the adjusted coefficient of determination of the model.**\n",
    "- **5.2) How would you write the linear regression model?**\n",
    "- **5.3) Are the fitted coefficients statistically significant at the 5% significance level?**\n",
    "- **5.4) How would you obtain the confidence intervals?**\n",
    "- **5.5) How do you interpret the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1) $R^2 \\approx \\bar{R}^2 = 0.787$.\\\n",
    "5.2) The (simple) linear regression model writes $\\hat{y} = \\beta_0 + \\beta_1x = -1.418e^{04} + 3160.21 x$, where $\\hat{y}$ is the predicted diamond price.\\\n",
    "5.3) The p-values associated to $\\beta_0$ and $\\beta_1$ are lower than 0.05, so both coefficients are statistically significant.\\\n",
    "5.4) The CI for $\\beta_0$ is $[-1.43e^{4}, -1.41e^{4}]$, and the CI for $\\beta_1$ is $[3146.34, 3174.08]$.\\\n",
    "5.5) A unit increase in the variable 'x' increases the average diamond price by 3160.21 US dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Compute the linear regression coefficients and intercept using the formula $\\hat{\\beta} = (X^T X)^{-1} X^T y$. Check that the obtained coeffients are the same than in the previous exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Fit the diamonds' price to the variable 'cut'. Note that 'cut' is a categorical variable, which we learned to preprocess in previous labs. When calling `dmatrices`, categorical variables will automatically be converted into dummy variables. You can also use the syntax [`C(variable)`](https://patsy.readthedocs.io/en/latest/categorical-coding.html) for more flexibility.**\n",
    "- **7.1) Does the model appear to be a better fit than with the variable 'x'?**\n",
    "- **7.2) How would you write the linear regression model now?**\n",
    "- **7.3) Are all coefficients statistically significant at the 5% significance level?**\n",
    "- **7.4) How do you interpret the model ? Does the intercept have a meaning?**\n",
    "- **7.5) Using `sns.boxenplot`, compare the distributions of the variable 'price' for the different values of 'cut'. Is it coherent to the model results?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1) $R^2$ dropped compared to the linear regression model using 'x' as predictor. This suggests that the variable 'cut' is a poorer predictor of the diamond's prices.\\\n",
    "7.2) $\\hat{y} = 4357.5 -431.10C_G -900.50C_I + 221.63C_P -375.67C_{VG}$.\\\n",
    "7.3) Yes, all p-values are smaller than 0.05.\\\n",
    "7.4) The average price for a 'Good' cut diamond is 4357.5 - 431.10 = 3926.4 US dollars. The average price for a 'Ideal' cut diamond is 4357.5 - 900.5 = 3457 US dollars, etc... Here the intercept represents the average price for a 'Fair' cut Diamond (which is the dropped category of the variable 'cut'), i.e. the average price of a 'Fair' cut diamond is 4357.5 US dollars.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.5) We observe some difference between the different distributions. Notably, premium diamonds tend to be the most expensive. However, the differences are not significant enough to predict the price based on the cut accurately, which might explain why 'cut' is a poor predictor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) Fit a linear regression model to the variable 'price' using 'x' and 'cut' as predictors. Based on [patsy's formulas](https://patsy.readthedocs.io/en/latest/formulas.html), add an interaction term between 'x' and 'cut'.**\n",
    "- **8.1) How does the model write now?**\n",
    "- **8.2) How do you interpret the model for a 'Good' cut diamond?**\n",
    "- **8.3) Are all coefficients statistically significant ? If no, what does it mean?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1) $\\hat{y} = -1.542e^{04} + 1449.67C_G + 1168.73C_I + 546.55C_P + 1227.94C_{VG} + 3163.25x - 99.69 x \\times C_G + 50.95 x \\times C_I + 92.33 x \\times C_P + 1.43 x \\times C_{VG}$.\n",
    "\n",
    "8.2) For a 'Good' cut diamond, a unit increase in 'x' leads the average price of a diamond to increase by $3163.25 -99.69 = 3063.56$ US dollars. Also, the average price of a 'Good' cut diamond is $-1.542e^{04} + 1449.67 + 3163.25x -99.69x = 1.397e^{04} - 3063.56x$ Us dollars. \n",
    "\n",
    "8.3) The coefficients associated to all interactions terms are not statistically significant at the 0.05 significance level (given that their p-values > 0.05). For these terms, we can only say that there is not enough evidence in the data to state that their associated coefficients are significantly different from 0. However, it does not mean that they are equal to 0, we simply cannot be certain that they are.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Fit a linear regression model to the variable 'price' using all predictors.**\n",
    "- **9.1) Derive the value of the Bayesian Information Criterion (BIC) and check that it corresponds to the value reported by the model's summary**  \n",
    "- **9.2) Does the model appear to be a better fit than the ones using only 'x' and 'cut' as predictors? Which criterion would you look at?**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2) $R^2$ increased compared to the models considering only 'x' or 'cut' (and their interaction terms). This is expected, as increasing the number of predictors tend to increase the value $R^2$ even if these predictors are irrelevant to the variable 'price'. Better criterion are the AIC or BIC, which penalize the inclusion of unnecessary predictors. In our case, both AIC and BIC decreased, so a model including all predictors appear to be a better fit to the variable 'price' than the models only considering 'cut' and 'x' as predictors. \n",
    "\n",
    "Keep in mind that $R^2$ can only be employed to compare models of equal size (i.e. that contain the same number of predictor variables)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10) The function `forward_selection` below implements a forward stepwise selection of predictors for linear regression. Make sure to understand each of its steps, and how they relate to the algorithm seen in class.**\n",
    "\n",
    "**Using this function, find the best subset of predictors using the BIC as selection critierion. Redo the same by using the AIC and $R^2$ as selection criterion. Do you notice any differences ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(df, target, criterion='BIC'):\n",
    "    assert criterion in ['BIC', 'AIC', 'R2'], 'Unknown criterion !'\n",
    "    # Iteratively, select the best feature to add to the model\n",
    "    candidates = set(df.columns) - {target}\n",
    "    ordered_predictors = []\n",
    "    while len(candidates) > 0:\n",
    "        rsquared_dict = {}\n",
    "        for predictor in candidates:\n",
    "            res = fit(df, ordered_predictors + [predictor], target)\n",
    "            rsquared_dict[predictor] = res.rsquared\n",
    "        best_predictor = max(rsquared_dict, key=rsquared_dict.get)\n",
    "        ordered_predictors = ordered_predictors + [best_predictor]\n",
    "        candidates.remove(best_predictor)\n",
    "    print(f'Best to worst predictors: {ordered_predictors}')\n",
    "    # Compute the scores obtained by adding each feature\n",
    "    current_features = []\n",
    "    scores = []\n",
    "    for feature in ordered_predictors:\n",
    "        current_features += [feature]\n",
    "        res = fit(df, current_features, target)\n",
    "        if criterion == 'BIC':\n",
    "            scores.append(res.bic)\n",
    "        elif criterion == 'AIC':\n",
    "            scores.append(res.aic)\n",
    "        elif criterion == 'R2':\n",
    "            scores.append(res.rsquared)\n",
    "    # Select the features that optimize the criterion\n",
    "    if criterion in ['BIC', 'AIC']:\n",
    "        num_final_features_to_keep = np.argmin(scores)\n",
    "    else:\n",
    "        num_final_features_to_keep = np.argmax(scores)\n",
    "    final_features_to_keep = ordered_predictors[:num_final_features_to_keep + 1]\n",
    "    best_final_score = scores[num_final_features_to_keep]\n",
    "    return final_features_to_keep, best_final_score\n",
    "\n",
    "def fit(df, features_to_try, target):\n",
    "    formula = get_formula(features_to_try, target)\n",
    "    y, X = dmatrices(formula, data=df, return_type='dataframe')\n",
    "    mod = sm.OLS(y, X)\n",
    "    res = mod.fit()\n",
    "    return res\n",
    "\n",
    "def get_formula(features_to_try, target):\n",
    "    return target + ' ~ ' + ' + '.join(features_to_try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIC and BIC resulted in the same set of predictors, which includes all predictors except the noisy variables that we have added. This is expected, as the AIC and BIC penalize the inclusion of unnecessary predictors. The $R^2$ criterion resulted in a larger subset of predictors, which is also expected. This indicates that, except for the noise, all predictors are related to the price of a diamond and are not totally redundant. Usually, when the number of predictors grows substantially, performing a forward selection on the BIC will result in a smaller subset than on the AIC, which in turn will result in a smaller subset than on $R^2$. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfe0e2b9b67a2fcef00021a1d2a516837bff30cf713e434b27f739f4afd30381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
