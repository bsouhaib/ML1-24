{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2023-2024 - UMONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bootstrap."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll experiment with **the bootstrap**, a simple but powerful resampling method that allows to quantify the uncertainty associated with any statistic estimated from a population sample. Additionally, the bootstrap enables us to estimate the validation error of any learning model, similarly to cross-validation. \n",
    "\n",
    "The steps of the (non-parametric) boostrap procedure can be summarized as:\n",
    "- Start with a dataset $\\mathcal{D} = \\{z_i\\}_{i=1}^n$.\n",
    "- For $b=1,...,B$, do:\n",
    "  - Sample a dataset $\\mathcal{D}^{(b)}=\\{z_i^{\\ast~(b)}\\}_{i=1}^n$ with replacement from the original dataset $\\mathcal{D}$.\n",
    "  - Estimate the statistic of interest on $\\hat{\\theta}^{\\ast~(b)} = s(\\mathcal{D}^{(b)})$ (e.g. the mean, $\\hat{\\theta}^{\\ast~(b)} = \\frac{1}{n}\\sum_{i=1}^n z_i^{\\ast~(b)}$).\n",
    "- Compute the sampling distribution of the statistic $s$ from $\\{\\hat{\\theta}^{\\ast~(1)},...,\\hat{\\theta}^{\\ast~(B)}\\}$.\n",
    "\n",
    "The sampling distribution of the statistic $s$ can then be used to quantify the uncertainty associated with $\\hat{\\theta} = s(\\mathcal{D})$, the statistic's estimate from the original dataset $\\mathcal{D}$. To get some intuition of the concept, we will start by applying the bootstrap algorithm on some simulated data, whose mean and variance is known."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Generate a dataset $\\mathcal{D}$ with $n=100$ observations from a Normal distribution with $\\mu =6$ and $\\sigma=2$. Check the `np.random.normal` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_mean = 6\n",
    "true_std = 2\n",
    "# TODO: z = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at the CDFs of the population distribution and the empirical distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "z_plot = np.linspace(0, 12, 10000)\n",
    "ax.plot(z_plot, norm.cdf(z_plot, true_mean, true_std), label='True CDF')\n",
    "ax.plot(z_plot, np.mean(z <= z_plot[:, np.newaxis], axis=1), label='Empirical CDF')\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Compute the sample mean $\\bar{z}$ and sample standard deviation $\\bar{\\sigma}$ of the dataset $\\mathcal{D}$.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Implement the bootstrap algorithm defined above for $B=1000$. Our goal here is to quantify the uncertainty associated to the sample mean $\\bar{z} = \\frac{1}{n} \\sum_{i=1}^n z_i$ and standard deviation $s = \\frac{1}{n - 1} \\sum_{i=1}^n (z_i - \\bar{z})^2$ using a 90% confidence interval.**\n",
    "\n",
    "**First, resample $B$ datasets using `sklearn.utils.resample` and collect the sample mean and standard deviation for each dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) What is the average number of unique points per bootstrap sample (i.e., the number of points that have been sampled at least once)? Compute it by modifying the code above. You can use the function `np.unique`.**\n",
    "\n",
    "**Can you compute the expected number of unique points per bootstrap sample using the theory of the course?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) Let $\\hat{z}^{\\ast~(b)}$ denote the sample means obtained on each bootstrapped dataset. For a confidence level $1 - \\alpha$, a bootstrap empirical confidence interval for $\\bar{z}$ can be obtained as:**\n",
    "\n",
    "$$\\text{CI} = [q_{\\alpha/2}(\\hat{z}^{\\ast~(b)}), q_{1-\\alpha/2}(\\hat{z}^{\\ast~(b)})]$$\n",
    "\n",
    "**where $q_{\\alpha}(\\hat{z}^{\\ast~(b)})$ is the $\\alpha$-quantile of the sampling distribution of $\\hat{z}^{\\ast~(b)}$ (e.g. the median of $\\hat{z}^{\\ast~(b)}$ is $q_{0.5}(\\hat{z}^{\\ast~(b)})$).**\n",
    "\n",
    "**Check the method `np.quantile` to compute quantiles.**\n",
    "\n",
    "**Then, plot the sampling distribution of the mean (e.g., using `sns.histplot`) and add the true value, the sample estimate and the 90% confidence interval.**\n",
    "\n",
    "**Redo the same for the sample standard deviation $\\hat{\\sigma}$.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will now use bootstrapping to estimate the uncertainty associated to the mean of a sample drawn from an unknown distribution. To this end, we will reuse the 'Fish Market' dataset, and apply the bootstrap to the variable 'Height'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/fish_lab.csv', index_col=0)\n",
    "df = df.astype({'Species': 'category'})\n",
    "df = df.sample(frac=1)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Using the same procedure as before, compute $90\\%$ confidence intervals for the variable height.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Similarly to cross-validation, the bootstrap can be used to estimate the validation error of any learning model. To this end, perform the following steps:**\n",
    "- **Using the provided preprocessor, create a pipeline to apply the same preprocessing steps as in the previous lab followed by a linear regression model.** \n",
    "- **For $b=1,...,1000$:**\n",
    "    - **Sample a training dataset $\\mathcal{D}_{\\text{train}}^{(b)}$ with replacement from the original dataset $\\mathcal{D}$.**\n",
    "    - **Define a test dataset $\\mathcal{D}_{\\text{test}}^{(b)}$ containing the observations from $\\mathcal{D}$ that are not in $\\mathcal{D}_{\\text{train}}^{(b)}$.**\n",
    "    - **For $\\mathcal{D}_{\\text{train}}^{(b)}$ and $\\mathcal{D}_{\\text{test}}^{(b)}$, select 'Height' as the target variable, and the remaining variables as predictors.**\n",
    "    - **Fit the pipeline on $\\mathcal{D}_{\\text{train}}^{(b)}$.**\n",
    "    - **Predict on $\\mathcal{D}_{\\text{test}}^{(b)}$.**\n",
    "    - **Compute the $\\text{MSE}^{(b)}$ on $\\mathcal{D}_{\\text{test}}^{(b)}$.**\n",
    "    - **Put $\\text{MSE}^{(b)}$ in a list.**\n",
    "\n",
    "**With this procedure, we obtain the sampling distribution of the MSE.**\n",
    "- **Plot this sampling distribution using a histogram.**\n",
    "- **Add a point estimate $\\text{MSE} = \\frac{1}{B}\\sum_{b=1}^B \\text{MSE}^{(b)}$ on the plot.**\n",
    "- **Add a 90% upper bound for the MSE on the plot, which corresponds to an interval from $-\\infty$ to the quantile 0.9 of the sampling distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns='Height'), df[['Height']]\n",
    "\n",
    "cont_columns = X.select_dtypes(include=['float64']).columns\n",
    "cat_columns = X.select_dtypes(include=['category']).columns\n",
    "# Transformers for imputation\n",
    "cont_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "cat_pipeline = make_pipeline(cat_imputer, OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "\n",
    "# ColumnTransformer to apply transformations to the correct features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cont', cont_imputer, cont_columns),\n",
    "    ('cat', cat_pipeline, cat_columns)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can verify that we obtain a relatively similar test MSE with cross-validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(model, X, y, cv=3, scoring=['neg_mean_squared_error'], return_estimator=True, return_indices=True)\n",
    "test_mse_per_fold = -cv_results['test_neg_mean_squared_error']\n",
    "print(f'Test MSE: {test_mse_per_fold.mean():.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bfe0e2b9b67a2fcef00021a1d2a516837bff30cf713e434b27f739f4afd30381"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
