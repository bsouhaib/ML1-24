{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2023-2024 - UMONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection using scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H3g2zh1W2t_c"
   },
   "source": [
    "**During the last lab, we learned how to fit regression and classification models to a dataset with scikit-learn. However, by fixing the number of features and by fixing the model's hyperparameters beforehand, we restricted ourselves to a single model. By doing so, we omitted to explore a broader range of models, one of which might better explain the relationship between our input and target variables.**\n",
    "\n",
    "**In this lab, we'll experiment with the general methodology of model selection, meaning that we'll define a set of predefined models, and we'll retain the one that minimizes the out-of-sample error.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1U5VE1l3YtKA"
   },
   "source": [
    "**Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zkt8eCSmYkbz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "I_O6yz5QY8bH"
   },
   "source": [
    "**In this lab, we will work with the [Fish market](https://www.kaggle.com/datasets/vipullrathod/fish-market) dataset, which contains several characteristics about fish, such as their weights, lengths, and species.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/fish_lab.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Print the type of each column and change the type of 'Species' to `category`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TO0Fesd7Zp0C",
    "outputId": "4cadd64c-4be2-40b9-b981-0f81631c04ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will start by predicting the target 'Height' from the feature 'Weight'. We split the dataset into a training and test set following a 75/25 partition using the `train_test_split` function. To avoid data leakage, we will not look at the test dataset during model selection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[['Weight']], df['Height']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, test_size=0.25, shuffle=True, random_state=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ARI4jGJDZ24R"
   },
   "source": [
    "**2) What is the number of missing values? Using the `SimpleImputer` class of scikit-learn with the argument `strategy`, replace the missing values by the sample mean, computed only on the training dataset to avoid data leakage.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Generate a scatter plot of the two variables on the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "U_PEwHV0f14c"
   },
   "source": [
    "**4) We can see that a linear model would not be the best option to model the relationship between these variables. Instead of fitting a linear model, let's fit a polynomial model of specified degree.**\n",
    "- **scikit-learn provides various \"transformers\", which can transform the dataset for scaling, normalization, encoding categorical variables, filling missing values, and more. To perform polynomial regression, we will use the `PolynomialFeatures` class**.\n",
    "- **scikit-learn also allows to create pipeline using `Pipeline` or `make_pipeline`, which are compositions of transformers followed by any model. Our polynomial regression model is a pipeline which first creates new features and then fits a linear regression model.**\n",
    "\n",
    "**More information is available [here](https://scikit-learn.org/stable/data_transforms.html).**\n",
    "\n",
    "**Create your model using the given `PolynomialRegression` function with `degree=2`. Then, plot the predictions of the model conditional to 'Weight' between 0 and 1250.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(SimpleImputer(strategy='mean'), PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5) We will now evaluate our first model with 10-fold cross-validation using the [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) function. In this process, the dataset is divided into 10 folds. For each fold, `cross_validate` will fit the model on the 9 other folds and test it on the remaining fold.**\n",
    "- **Create a pipeline, composed of the `SimpleImputer` transformer from earlier and a `PolynomialRegression` model with degree 2.**\n",
    "- **For evaluation, we will use the mean squared error (MSE). Note that scikit-learn provides [different score functions](https://scikit-learn.org/stable/modules/model_evaluation.html), which should be maximized. This is in contrast to loss functions, which should be minimized. Thus, we will use the score `neg_mean_squared_error` (the negation of the MSE), and we should not forget to take the negation when reporting it.**\n",
    "- **Call `cross_validate` with 10 folds and specify the score function using the `scoring` argument. Since `X_train` and `y_train` have been modified previously with `SimpleImputer`, there will be data leakage between folds. Instead, use the given `X_train_raw` and `y_train_raw` as argument.**\n",
    "- **Report the test MSE on each fold based on the dictionary of metrics returned by [`cross_validate`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html). Then, report the mean test MSE across all fold.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We recreate the train and test sets because we modified the train set earlier.\n",
    "X, y = df[['Weight']], df['Height']\n",
    "\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y, train_size=0.75, test_size=0.25, shuffle=True, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1a-6zXngJgZ",
    "outputId": "6da8ea7c-de9e-402b-8414-53c4e1062508"
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aZm20YCJjb2j"
   },
   "source": [
    "**Let's now see how predictions vary with the model complexity. We go back to using the train and test sets defined earlier. For polynomial degrees between 1 and 5, we repeatedly fit the polynomial regression model (without cross-validation) and plot the predictions of 'Height' in function of the feature 'Weight'. What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "degrees = [1, 2, 3, 5, 8, 15]\n",
    "for degree in degrees:\n",
    "    model = PolynomialRegression(degree, fit_intercept=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    models.append(model)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_train, y_train, label='Train points', s=20)\n",
    "X_plot = pd.DataFrame({'Weight': np.linspace(0, 2000, 500)})\n",
    "for model in models:\n",
    "    degree = model.get_params()['polynomialfeatures__degree']\n",
    "    y_plot = model.predict(X_plot)\n",
    "    ax.plot(X_plot, y_plot, label=f'Degree {degree}')\n",
    "ax.set_xlim([X_train['Weight'].min() - 10, X_train['Weight'].max() + 10])\n",
    "ax.set_ylim([y_train.min() - 1, y_train.max() + 1])\n",
    "ax.set(xlabel='Weight', ylabel='Height')\n",
    "fig.legend(loc='lower center', bbox_to_anchor=(0.5, 1), frameon=True, ncol=4)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Collect and plot the evolution of the train and test MSE of the models from the previous question in function of the polynomial degree. Which model would you select?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial of degree 2 achieves the lowest test MSE and should be selected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the rest of this lab, we will predict the target 'Height' based on all available features. We split again the dataset into a training and test set following a 75/25 partition using the `train_test_split` function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns='Height'), df[['Height']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, test_size=0.25, shuffle=True, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) We will now create a more complex pipeline to preprocess the data:**\n",
    "- **For continuous variables, we replace the missing values by the sample mean.**\n",
    "- **For missing categorical variables, replace missing value by the sample mode (the most frequent value).**\n",
    "- **After replacing missing categorical variables, we replace them by dummy variables using the `OneHotEncoder`.**\n",
    "- **To execute different transformers on different columns, we can use `ColumnTransformer`**\n",
    "\n",
    "**For this, combine the transformers `SimpleImputer`, `OneHotEncoder` and `ColumnTransformer` into a single pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select continuous and categorical columns using this method\n",
    "cont_columns = X_train.select_dtypes(include=['float64']).columns\n",
    "cat_columns = X_train.select_dtypes(include=['category']).columns\n",
    "print('Continuous columns:', cont_columns)\n",
    "print('Categorical columns:', cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers for imputation\n",
    "cont_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "cat_pipeline = make_pipeline(cat_imputer, OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "\n",
    "# ColumnTransformer to apply transformations to the correct features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cont', cont_imputer, cont_columns),\n",
    "    ('cat', cat_pipeline, cat_columns)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8) Usually, models have more than one hyperparameter that can be tuned in order to find the model that best captures the relationship between our input and target variables. For instance, in the case of a simple linear regression using a polynomial transformation on the input variables, we can choose the hyperparameter space to be the polymial's degree, and whether or not to fit the intercept. Inspecting each combination of hyperparameters and selecting the combination that results in the best model is called grid search.**\n",
    "\n",
    "**scikit-learn provides the class [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), which evaluates each combination of hyperparameters using cross-validation. After fitting, we can get relevant attributes:**\n",
    "- **`cv_results_` returns a summary of the results and can be converted to a pandas dataframe`.**\n",
    "- **`best_index_` returns the index of `cv_results_` for the best model.**\n",
    "- **`best_params_` returns the parameters of the best model.**\n",
    "\n",
    "**Perform a grid search on the hyperparameter space of a polynomial regression model (with all features). Search for degrees varying between 1 and 5, and whether or not the intercept should be fit. Report the best hyperparameters and the corresponding MSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps surprisingly, the best score was obtained with a polynomial of degree 1. Although introducing more features offers greater flexibility to the model, it also raises the chance of the model fitting to noise within the features - an indication of overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9) Finally, fit a model using the best hyperparameters on the full training dataset. You can use the function `set_params` of scikit-learn [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) classes. Collect the predictions on the test set in a variable `y_pred` and report the test MSE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the MSE decreases when the full training dataset is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For visualization, we plot the predicted values of 'Height' as a function 'Weight'. Note that, due to the presence of other variables, the model is able to represent non-linear relationships between the two variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sort according to the x-axis for visualization purposes\n",
    "x_label = 'Weight'\n",
    "X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "X_test = X_test.sort_values(x_label)\n",
    "y_test = y_test.loc[X_test.index]\n",
    "y_pred = y_pred[X_test.index]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_test[x_label], y_test, color='green', label='Test points', s=20)\n",
    "ax.plot(X_test[x_label], y_pred, label='Best model found by cross-validation', color='r')\n",
    "ax.set(xlabel=x_label, ylabel='Height')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional experiments: try to improve the performance of the model by manually transforming the features. For example add the squared 'Weight' as a feature.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab4_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_tpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "af12c90e6e9333d91951ed5480a2e20e375976b7e40bdc3604f2bdd44ecfbb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
